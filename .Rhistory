all_data$ROA<-all_data$profit1 / (all_data$debt1 + all_data$netAsset1)
all_data$ROA<-ifelse(is.na(all_data$ROA), 0, all_data$ROA)
sort(all_data$ROA)
all_data$ROA<-ifelse(all_data$ROA==Inf, 1, ifelse(all_data$ROA==-Inf,-1,all_data$ROA ))
all_data$ROE <- all_data$profit1 / all_data$netAsset1
all_data$ROE<-ifelse(is.na(all_data$ROE), 0, all_data$ROE)
sort(all_data$ROE)
all_data$ROE<-ifelse(all_data$ROE==Inf, 1, ifelse(all_data$ROE==-Inf,-1,all_data$ROE ))
write.csv(all_data2,"cleaning_data2.csv",row.names = F)
all_data <-read.csv("cleaning_data2.csv")
colnames(all_data)
str(all_data)
all_data$instkind<-as.factor(all_data$instkind)
#더미변수화
str(all_data)
colnames(all_data)
all_data2<- all_data
dum=model.matrix(~instkind+ownerChange,all_data2)
colnames(all_data)
dum=model.matrix(~instkind+ownerChange+sido,all_data2)
colnames(dum)
dum1=model.matrix(~instkind,all_data2)
colnames(dum1)<-paste(colnames(dum1),"_1",sep = "")
dum2=model.matrix(~ownerChange,all_data2)
colnames(dum2)<-paste(colnames(dum2),"_2",sep = "")
dum3=model.matrix(~sido,all_data2)
colnames(dum3)<-paste(colnames(dum3),"_3",sep = "")
dum<-cbind(dum1,dum2,dum3)
colnames(dum)
colnames(all_data2)
all_data2= all_data2 %>% select(-c(sido,ownerChange,instkind))
all_data2 =cbind(all_data2,dum)
##########기본패키지 및 설정 #####################
setwd("C:/Users/Admin/Desktop/끝난플젝/2018ZPER 2ndCompetitionData")
rm(list = ls())
gc()
library(caret)
library(dplyr)
library(reshape)
library(Amelia)
require(mice)
library(ROSE)
library(DMwR)
library(devtools);#install_github("RomeroBarata/bimba",force=TRUE)
library(caret)
library(xgboost)
library(kernlab)
library(e1071)
library(data.table)
### 정규화/factor to 더미변수화 및 train/test쪼개기###########
#정규화
all_data <-read.csv("cleaning_data2.csv")
colnames(all_data)
str(all_data)
all_data$instkind<-as.factor(all_data$instkind)
all_data2<- all_data
dum1=model.matrix(~instkind,all_data2)
colnames(dum1)<-paste(colnames(dum1),"_1",sep = "")
dum2=model.matrix(~ownerChange,all_data2)
colnames(dum2)<-paste(colnames(dum2),"_2",sep = "")
dum3=model.matrix(~sido,all_data2)
colnames(dum3)<-paste(colnames(dum3),"_3",sep = "")
dum<-cbind(dum1,dum2,dum3)
colnames(dum)
colnames(all_data2)
all_data2= all_data2 %>% select(-c(sido,ownerChange,instkind))
all_data2 =cbind(all_data2,dum)
#쪼개기
train =all_data2[1:293,]
test =all_data2[294:420,]
#raw data
train =read.csv("train.csv")
###0 원본데이터 및 전처리 #########################################
rm(list = ls())
gc()
train =read.csv("train.csv")
test =read.csv("test.csv")
table(train$OC) #tartget값이 매우 불균형함 .
str(train)
str(test)
###1 이상한 데이터& 결측치 처리 #####################
#결측치 확인
sum(is.na(train)) #424
colSums(is.na(train)) %>% as.data.frame()
length(which(is.na(train$debt1))) #8
length(which(is.na(train$ownerChange))) # 12
length(which(is.na(train$bedCount))) # 5
length(which(is.na(train$employee2))) # 13
length(which(is.na(train$employee1))) # 10
sum(is.na(test)) #247개
colSums(is.na(test)) %>% as.data.frame()
which(is.na(test$debt1)) # 2
sum(is.na(test$openDate)) #1
sum(is.na(test$bedCount)) #8
sum(is.na(test$ownerChange)) #15
#->  특정변수의 경우 결측치가 매우 많음
str(train)
str(test)
# "" 데이터 처리 -> NA로
#test의 employee1,2는 factor형태
train$instkind = ifelse(train$instkind=="",NA,train$instkind)
test$instkind = ifelse(test$instkind=="",NA,test$instkind) # 2개
test$employee1 = ifelse(test$employee1==""|test$employee1=="_",NA,test$employee1)
test$employee2 = ifelse(test$employee1==""|test$employee2=="_",NA,test$employee2)
test$employee1<-test$employee1 %>% as.character() %>% as.integer()
test$employee2<-test$employee2 %>% as.character() %>% as.integer()
str(train)
str(test)
sum(is.na(train))
sum(is.na(test))
#train 재무제표 결측치 제거
#train %>% View()
missmap(train) #결측치 시각화 결과 revenue1가 NA인 경우, 나머지 재무제표 요소들도 NA임
#따라서 revenue1가 NA인 데이터를 제거
train1 =train[-c(which(is.na(train$revenue1))),]
missmap(train1) #결측치가 거의 제거된 것을 확인
colSums(is.na(train1)) %>% as.data.frame()
#나머지 결측치 ->  bedCount:5 / instkind:1 / employee1:8 /employee2:11 / ownerChange:10
#bedcount / employee1 / employee2의 경우 NA를 0으로 대체
#ownerchange의 경우 범주형으로  "X"라는 범주를 생성
#instkinds는 보다 관찰 후 mice패키지로 채움.
train1[is.na(train1$bedCount),]$bedCount <-0
train1[is.na(train1$employee1),]$employee1 <-0
train1[is.na(train1$employee2),]$employee2 <-0
train1$ownerChange<-train1$ownerChange %>% as.character()
train1$ownerChange<- ifelse(is.na(train1$ownerChange),"non",train1$ownerChange) %>% as.factor()
missmap(train1)
#mice
imp<-mice(train1,method = "cart",seed=1234)
train_mice<-complete(imp)
missmap(train_mice)
str(train_mice)
#test 재무제표 결측치 -> 0으로 처리 (test셋은 제거 불가능 )
colnames(test[,7:54])
missmap(test)
test<-test %>% select(-OC) #test OC행 결측치 제거
colSums(is.na(test)) %>% as.data.frame()
test[,7:54][is.na(test[,7:54])]<-0
#test 행 결측치
missmap(test)
colSums(is.na(test)) %>% as.data.frame()
#openDate:1 / bedCount:8 / instkind:2 / employee1:8 / employee2:8 / ownerChange:15
test[is.na(test$bedCount),]$bedCount <-0
test[is.na(test$employee1),]$employee1 <-0
test[is.na(test$employee2),]$employee2 <-0
test$ownerChange<-test$ownerChange %>% as.character()
test$ownerChange<- ifelse(is.na(test$ownerChange),"non",test$ownerChange) %>% as.factor()
imp<-mice(test,method = "cart",seed=1234)
test_mice<-complete(imp)
missmap(test_mice)
#md.pattern(train)
sum(is.na(train_mice))
sum(is.na(test_mice))
#http://blog.naver.com/PostView.nhn?blogId=mokeya1&logNo=221337295488&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView
##mice sample
# mice(data[,-2],m=5,maxit=10,method='cart',seed=1234)
#mice_train <- mice(train[,-1],m=5,maxit=10,meth='cart',seed=1234) #나중에 m, maxit 조정
#train_re = complete(mice_train,3) #N개중 하나만, 이 후 각각의 데이터의 앙상블도 결과 나쁘지 않음, or 평균으로
#save(mice_train, file="mice_train.RData")
#save(mice_test, file="mice_test.RData")
#save(mice_train, file="mice_train2.RData")
#save(mice_test, file="mice_test2.RData")
#load(file="mice_train2.RData")
#load(file="mice_test2.RData")
str(train_mice)
str(test_mice)
target<-train_mice$OC
train_mice<-train_mice %>% select(-OC)
dim(train_mice); dim(test_mice)
#compare two dataframe column class
m <- sapply(list(df1 = train_mice, df2 = test_mice), sapply, class)
m[m[, "df1"] != m[, "df2"],]
# 합치기 전에 전처리
train_mice$openDate<-train_mice$openDate %>% as.numeric()
train_mice$inventoryAsset1<-train_mice$inventoryAsset1 %>% as.numeric()
train_mice$receivableL1<-train_mice$receivableL1 %>% as.numeric()
train_mice$inventoryAsset2<-train_mice$inventoryAsset2 %>% as.numeric()
train_mice$receivableL2<-train_mice$receivableL2 %>% as.numeric()
all_data<-rbind(train_mice,test_mice)
str(all_data)
sum(is.na(all_data))
#결측치 제거한 데이터
write.csv(target,"target.csv",row.names = F)
###0 원본데이터 및 전처리 #########################################
rm(list = ls())
gc()
### 정규화/factor to 더미변수화 및 train/test쪼개기###########
#정규화
all_data <-read.csv("cleaning_data2.csv")
target<-read.csv("target.csv")
colnames(all_data)
str(target)
View(target)
all_data$instkind<-as.factor(all_data$instkind)
str(all_data)
colnames(all_data)
all_data2<- all_data
dum1=model.matrix(~instkind,all_data2)
colnames(dum1)<-paste(colnames(dum1),"_1",sep = "")
dum2=model.matrix(~ownerChange,all_data2)
colnames(dum2)<-paste(colnames(dum2),"_2",sep = "")
dum3=model.matrix(~sido,all_data2)
colnames(dum3)<-paste(colnames(dum3),"_3",sep = "")
dum<-cbind(dum1,dum2,dum3)
colnames(dum)
colnames(all_data2)
all_data2= all_data2 %>% select(-c(sido,ownerChange,instkind))
all_data2 =cbind(all_data2,dum)
#쪼개기
train =all_data2[1:293,]
test =all_data2[294:420,]
train$OC<-target$x
dim(train)
dim(test)
str(train)
colnames(train)
colnames(test)
str(train$OC)
### 변수 선택 #########################
library(mlbench)
zv =nearZeroVar(train, saveMetrics=TRUE) #nzv이 TRUE인 것 제거
zv$nzv
colnames(train[zv$nzv])
##under sampling
table(train$OC)
da1 = train[train$OC!="open",] #15개 close 데이터
da2 =train[train$OC=="open",]
cl = train[train$OC!="open",] #15개 close 데이터
op=train[train$OC=="open",]
idx = sample(1:nrow(op), 90)
op = op[idx, ]
train1 =rbind(c1,op)
str(train1)
train1 =rbind(cl,op)
str(train1)
table(train1$OC)
##under sampling
table(train$OC)
###ROSE -skip
###SMOTE
table(train$OC)
sample =SMOTE(OC~.,data =train,
perc.over=1000 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(sample$OC)
str(sample)
sample =SMOTE(OC~.,data =train,
perc.over=200 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(sample$OC) #300 / 165
sample =SMOTE(OC~.,data =train,
perc.over=500 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(sample$OC) #300 / 165
sample =SMOTE(OC~.,data =train,
perc.over=1000 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(sample$OC) #300 / 165
sample =SMOTE(OC~.,data =train,
perc.over=900 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(sample$OC) #300 / 165
### use validation (to smote) #####################
str(train)
### use validation (to smote) #####################
colnames(train)
a<-createDataPartition(train$OC,p=0.3)
train[a,]
train[a]
a
train[a,]
idx<- createDataPartition(train$OC, p = 0.3, list = FALSE)
val_train<-train[ind,]
val_train<-train[idx,]
val_train<-train[-idx,]
val_test<-train[idx,]
table(val_train$OC)
table(val_test$OC)
#xgb factor to numeric
#preparing matrix
set.seed(1234)
control <- trainControl(method="repeatedcv",
number=10 ,
repeats=2,
#summaryFunction = twoClassSummary,
classProbs = TRUE
)
tune_grid <- expand.grid(nrounds = 300,
eta =0.05,
max_depth = c(4, 6, 8),
gamma = c(1, 3),
subsample = 0.5,
min_child_weight = 1,
colsample_bytree = 1
)
levels(train$OC) <- make.names(levels(factor(train$OC)))
str(train)
val_sample =SMOTE(OC~.,data =val_train,
perc.over=900 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지 (n+1배 )
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
table(val_sample)
table(val_sample$OC)
table(val_train$OC)
str(val_train)
xgb_fit <- train(OC ~., data = val_train,
method = "xgbTree",
objective = "binary:logistic",
#names(getModelInfo())
trControl=control,
tuneGrid = tune_grid
#preProcess =
#tuneLength = 10
)
##---
str(val_train)
##---
sapply(all_data, class)
##---
sapply(all_data, class) %>% as.data.frame()
rm(list = ls())
gc()
### 정규화/factor to 더미변수화 및 train/test쪼개기###########
#정규화
all_data <-read.csv("cleaning_data2.csv")
target<-read.csv("target.csv")
colnames(all_data)
str(all_data)
str(target)
all_data$instkind<-as.factor(all_data$instkind)
sapply(all_data,is.factor)
all_data[,a]
a<-sapply(all_data,is.factor)
all_data[,a]
all_data[a,]
a
sapply(all_data,is.factor) %>% as.data.frame()
str(all_data)
colnames(all_data)
all_data2<- all_data
dum1=model.matrix(~instkind,all_data2)
colnames(dum1)<-paste(colnames(dum1),"_1",sep = "")
dum2=model.matrix(~ownerChange,all_data2)
colnames(dum2)<-paste(colnames(dum2),"_2",sep = "")
dum3=model.matrix(~sido,all_data2)
colnames(dum3)<-paste(colnames(dum3),"_3",sep = "")
dum<-cbind(dum1,dum2,dum3)
colnames(dum)
colnames(all_data2)
all_data2= all_data2 %>% select(-c(sido,ownerChange,instkind))
all_data2 =cbind(all_data2,dum)
#쪼개기
train =all_data2[1:293,]
test =all_data2[294:420,]
train$OC<-target$x
### use validation (to smote) #####################
str(all_data2)
#쪼개기
train =all_data2[1:293,]
test =all_data2[294:420,]
train$OC<-target$x
colnames(train)
sapply(train,is.factor) %>% as.data.frame()
idx<- createDataPartition(train$OC, p = 0.3, list = FALSE)
val_train<-train[-idx,]
val_test<-train[idx,]
val_sample =SMOTE(OC~.,data =val_train,
perc.over=900 ,# 적은 쪽의 데이터를 얼마나 추가로 샘플링해야 하는지 (n+1배 )
k=5           # 고려할 최근접 이웃의 수
# 적은 쪽의 데이터를 추가로 샘플링할 때 각 샘플에 대응해서 많은 쪽의 데이터를
# 얼마나 샘플링할지 지
)
##1. 기존의 불균형한 val_train으로
control <- trainControl(method="repeatedcv",
number=10 ,
repeats=2,
#summaryFunction = twoClassSummary,
classProbs = TRUE
)
tune_grid <- expand.grid(nrounds = 300,
eta =0.05,
max_depth = c(4, 6, 8),
gamma = c(1, 3),
subsample = 0.5,
min_child_weight = 1,
colsample_bytree = 1
)
levels(val_train$OC) <- make.names(levels(factor(val_train$OC)))
str(val_train)
xgb_fit <- train(OC ~., data = val_train,
method = "xgbTree",
objective = "binary:logistic",
#names(getModelInfo())
trControl=control,
tuneGrid = tune_grid
#preProcess =
#tuneLength = 10
)
colnames(val_test)
a=predict(xgb_fit,val_test %>% select(-OC));
confusionMatrix(a,val_test$OC)
a
val_test$OC
confusionMatrix(a,val_test %>% select(OC))
str(a)
str(val_test$OC)
str(val_train$OC)
library(plyr) #factor level 변경
str(a)
revalue(a, c("X.close"="cloase"))
str(a)
a<-revalue(a, c("X.close"="cloase"))
str(a)
a=predict(xgb_fit,val_test %>% select(-OC));
str(a)
a<-revalue(a, c("X.close"="close"))
confusionMatrix(a,val_test %>% select(OC))
str(a)
str(val_test$OC)
val_test$OC<-revalue(val_test$OC,c(" close"="close"))
confusionMatrix(a,val_test %>% select(OC))
str(a)
str(val_test$OC)
confusionMatrix(a,val_test %>% select(OC))
table(a)
table(b)
table(val_test$OC)
confusionMatrix(table(a,val_test %>% select(OC)))
table(a,val_test$OC)
table(a,val_test$OC) %>% confus
table(a,val_test$OC) %>% confusionMatrix()
###2. SMTOE: val_sample으로
control <- trainControl(method="repeatedcv",
number=10 ,
repeats=2,
#summaryFunction = twoClassSummary,
classProbs = TRUE
)
tune_grid <- expand.grid(nrounds = 300,
eta =0.05,
max_depth = c(4, 6, 8),
gamma = c(1, 3),
subsample = 0.5,
min_child_weight = 1,
colsample_bytree = 1
)
#levels(val_train$OC) <- make.names(levels(factor(val_train$OC)))
str(val_sample)
start<-Sys.time()
start<-Sys.time()
xgb_fit <- train(OC ~., data = val_sample,
method = "xgbTree",
objective = "binary:logistic",
#names(getModelInfo())
trControl=control,
tuneGrid = tune_grid
#preProcess =
#tuneLength = 10
)
end<-Sys.time()
levels(val_sample$OC) <- make.names(levels(factor(val_sample$OC)))
start<-Sys.time()
xgb_fit <- train(OC ~., data = val_sample,
method = "xgbTree",
objective = "binary:logistic",
#names(getModelInfo())
trControl=control,
tuneGrid = tune_grid
#preProcess =
#tuneLength = 10
)
end<-Sys.time()
end-start
a=predict(xgb_fit,val_test %>% select(-OC));
table(a,val_test$OC) %>% confusionMatrix() #기존 : 0.9326
### confusion matrix
str(a)
table(a,val_test$OC) %>% confusionMatrix() #기존 : 0.9326
table(a,val_test$OC)
a<-revalue(a,c("X.cloase"="cloase"))
a<-revalue(a,c("X.close"="cloase"))
### confusion matrix
str(a)
### confusion matrix
str(a)
a<-revalue(a,c("X.close"="close"))
a=predict(xgb_fit,val_test %>% select(-OC));
a<-revalue(a,c("X.close"="close"))
a<-revalue(a,c("X.close"="close"))
### confusion matrix
str(a)
str(val_test$OC)
table(a,val_test$OC) %>% confusionMatrix() #기존 : 0.9326
rm(list = ls())
gc()
