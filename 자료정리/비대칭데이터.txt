비대칭 데이터 처리

https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/



비용에 민감한 학습
정규 학습에서 우리는 모든 잘못 분류를 동등하게 다루므로 분류 문제의 불균형 문제를 야기합니다. 대다수 클래스보다 소수 분류를 식별하는 데 추가 보상이 없기 때문입니다. 비용에 민감한 학습은이를 변경하고 클래스 p의 인스턴스를 클래스 p로 잘못 분류하는 비용을 지정하는 함수 C (p, t) (대개 행렬로 표시)를 사용합니다. 이를 통해 소수 계층의 잘못된 분류에 대해 대다수 계층의 오 분류와 비교할 때 진정한 긍정적 인 비율이 증가하기를 바라면서 벌칙을 부과 할 수 있습니다. 이에 대한 공통적 인 계획은 비용이 클래스가 구성하는 데이터 세트의 비율의 역수와 같도록하는 것입니다. 이는 학급 규모가 감소함에 따라 벌칙을 증가시킵니다.

이상 탐지
극단적 인 경우에는 이상 탐지와 관련하여 분류를 생각하는 것이 좋습니다. 이상 탐지에서 우리는 데이터 포인트의 "정상적인"분포가 있다고 가정하고 그 분포에서 충분히 벗어나는 것은 예외입니다. 분류 문제를 비정상 탐지 문제로 재구성 할 때 대다수 클래스를 "정상"분포로, 소수를 비정상으로 취급합니다. 클러스터링 방법, One-class SVM 및 Isolation Forest와 같은 변형 탐지를위한 알고리즘이 많이 있습니다.

결론
이러한 방법을 조합하면 더 나은 분류 기준을 만들 수 있기를 바랍니다. 전에 말했듯이, 이러한 기술 중 일부는 서로 다른 불균형의 정도에 도움이됩니다. 예를 들어 간단한 샘플링 기법을 사용하면 약간의 불균형을 극복 할 수 있지만 극단적 인 불균형에는 예외 탐지 방법이 필요할 수 있습니다. 궁극적으로이 문제에 대해 모든 방법에 맞는 단일 방법이 없으므로 각 방법을 시험해보고 성공 사례를 특정 유스 케이스 및 메트릭에 적용하는 방법을 확인하기 만하면됩니다.


https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes

##xgb 
https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/

https://rdrr.io/github/RomeroBarata/bimba/ 패키지 
ADASYN
Borderline-SMOTE